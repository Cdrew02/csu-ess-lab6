[
  {
    "objectID": "Index.html",
    "href": "Index.html",
    "title": "Index",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.0.4     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.3.0 â”€â”€\nâœ” broom        1.0.7     âœ” rsample      1.2.1\nâœ” dials        1.4.0     âœ” tune         1.3.0\nâœ” infer        1.0.7     âœ” workflows    1.2.0\nâœ” modeldata    1.4.0     âœ” workflowsets 1.1.0\nâœ” parsnip      1.3.1     âœ” yardstick    1.3.2\nâœ” recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'infer' was built under R version 4.4.3\n\n\nWarning: package 'modeldata' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'workflowsets' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard() masks purrr::discard()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– recipes::fixed()  masks stringr::fixed()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– yardstick::spec() masks readr::spec()\nâœ– recipes::step()   masks stats::step()\n\nlibrary(tidypredict)\n\nWarning: package 'tidypredict' was built under R version 4.4.3\n\nlibrary(nnet)\n\nWarning: package 'nnet' was built under R version 4.4.3\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\n\n\nlibrary(tidyverse)\nlibrary(glue)\n\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'  # ðŸ”§ Define root BEFORE glue\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_clim.txt: cannot open destfile\n'data/camels_clim.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_geol.txt: cannot open destfile\n'data/camels_geol.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_soil.txt: cannot open destfile\n'data/camels_soil.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_topo.txt: cannot open destfile\n'data/camels_topo.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_vege.txt: cannot open destfile\n'data/camels_vege.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt: cannot open\ndestfile 'data/camels_hydro.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n#question 1: zero_q_freq represents the percentage of days with zero streamflow (0 mm/day) in a given period, providing insight into hydrological extremes and water behavior in the drainage area.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.4.3\n\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nlibrary(patchwork)\n\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Map for aridity\nmap_aridity &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) + \n  geom_point(aes(color = aridity)) + \n  scale_color_viridis_c() +\n  theme_minimal() + \n  labs(title = \"Aridity of Sites\", \n       color = \"Aridity\")\n\n# Map for p_mean (mean rainfall)\nmap_p_mean &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) + \n  geom_point(aes(color = p_mean)) + \n  scale_color_viridis_c() +\n  theme_minimal() + \n  labs(title = \"Mean Rainfall (p_mean) of Sites\", \n       color = \"Mean Rainfall\")\n\n# Combine maps using patchwork\nlibrary(patchwork)\ncombined_maps &lt;- map_aridity + map_p_mean + plot_layout(ncol = 1)\n\n# Display the combined maps\nprint(combined_maps)\n\n\n\n\n\n\n\n\n\n# Load libraries\nlibrary(tidymodels)\nlibrary(baguette)    # For neural network (bag_mlp)\nlibrary(xgboost)     # For XGBoost model\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\n# Set seed for reproducibility\nset.seed(123)\n\n# --- Data Preparation ---\n# Log-transform target variable 'q_mean' to create 'logQmean'\ncamels &lt;- camels %&gt;% mutate(logQmean = log(q_mean))\n\n# Split data: 80% for training, 20% for testing\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n# 5-fold cross-validation for faster processing\ncamels_cv &lt;- vfold_cv(camels_train, v = 5)\n\n# --- Preprocessing Recipe ---\n# Predict logQmean using aridity and p_mean\n# Log-transform predictors and add an interaction term\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%               # Log-transform predictors\n  step_interact(terms = ~ aridity:p_mean) %&gt;%   # Add interaction term\n  step_naomit(all_predictors(), all_outcomes()) # Remove rows with missing data\n\n# --- Model Specifications ---\n# 1. Linear Regression (baseline)\nlm_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# 2. Random Forest Model\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# 3. XGBoost Model\nxgb_model &lt;- boost_tree(\n  trees = 1000,   # Number of trees\n  tree_depth = 6, # Max depth of trees\n  min_n = 10,     # Min observations per node\n  learn_rate = 0.01 # Learning rate\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# 4. Neural Network (bag_mlp) Model\nnn_model &lt;- bag_mlp(\n  hidden_units = 100, # Number of hidden units\n  epochs = 50         # Reduced epochs for quicker testing\n) %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n# --- Create Workflow Set ---\n# Combine the recipe and all models into a workflow set\nwf_set &lt;- workflow_set(\n  preproc = list(rec),\n  models  = list(\n    lm  = lm_model,\n    rf  = rf_model,\n    xgb = xgb_model,\n    nn  = nn_model\n  )\n)\n\n# --- Fit Models with Resampling ---\n# Use 5-fold CV to fit each model in the workflow\nwf_res &lt;- wf_set %&gt;% \n  workflow_map(\"fit_resamples\", \n               resamples = camels_cv, \n               seed = 123, \n               verbose = TRUE)\n\ni 1 of 4 resampling: recipe_lm\n\n\nâœ” 1 of 4 resampling: recipe_lm (421ms)\n\n\ni 2 of 4 resampling: recipe_rf\n\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\nâœ” 2 of 4 resampling: recipe_rf (1.1s)\n\n\ni 3 of 4 resampling: recipe_xgb\n\n\nâœ” 3 of 4 resampling: recipe_xgb (3.2s)\n\n\ni 4 of 4 resampling: recipe_nn\n\n\nâœ” 4 of 4 resampling: recipe_nn (16.4s)\n\n# --- Evaluate Model Performance ---\n# Collect metrics (e.g., RMSE, R-squared, MAE)\nmodel_metrics &lt;- wf_res %&gt;% collect_metrics()\nprint(model_metrics)\n\n# A tibble: 8 Ã— 9\n  wflow_id   .config        preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_lm  Preprocessor1â€¦ recipe  lineâ€¦ rmse    standard   0.575     5  0.0220\n2 recipe_lm  Preprocessor1â€¦ recipe  lineâ€¦ rsq     standard   0.765     5  0.0216\n3 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rmse    standard   0.563     5  0.0241\n4 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rsq     standard   0.773     5  0.0278\n5 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rmse    standard   0.555     5  0.0197\n6 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rsq     standard   0.781     5  0.0217\n7 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rmse    standard   0.531     5  0.0289\n8 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rsq     standard   0.795     5  0.0309\n\n# Rank models based on RMSE (lower is better) or R-squared (higher is better)\nranked_results &lt;- rank_results(wf_res, rank_metric = \"rmse\", select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 8 Ã— 9\n  wflow_id   .config        .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn  Preprocessor1â€¦ rmse    0.531  0.0289     5 recipe       bag_â€¦     1\n2 recipe_nn  Preprocessor1â€¦ rsq     0.795  0.0309     5 recipe       bag_â€¦     1\n3 recipe_xgb Preprocessor1â€¦ rmse    0.555  0.0197     5 recipe       boosâ€¦     2\n4 recipe_xgb Preprocessor1â€¦ rsq     0.781  0.0217     5 recipe       boosâ€¦     2\n5 recipe_rf  Preprocessor1â€¦ rmse    0.563  0.0241     5 recipe       randâ€¦     3\n6 recipe_rf  Preprocessor1â€¦ rsq     0.773  0.0278     5 recipe       randâ€¦     3\n7 recipe_lm  Preprocessor1â€¦ rmse    0.575  0.0220     5 recipe       lineâ€¦     4\n8 recipe_lm  Preprocessor1â€¦ rsq     0.765  0.0216     5 recipe       lineâ€¦     4\n\n# --- Extract the Best Model ---\n# Assume XGBoost is the best based on RMSE (adjust if necessary)\nbest_wf &lt;- wf_res %&gt;%\n  extract_workflow(\"recipe_xgb\") %&gt;%  # Replace with the best model id if different\n  fit(data = camels_train)\n\n# --- Make Predictions on Test Data ---\nbest_preds &lt;- predict(best_wf, new_data = camels_test) %&gt;% \n  bind_cols(camels_test)\n\n# Evaluate the best model's performance on the test data\ntest_metrics &lt;- metrics(best_preds, truth = logQmean, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.652\n2 rsq     standard       0.694\n3 mae     standard       0.389\n\n# --- Visualize: Observed vs. Predicted ---\n# Plot observed vs predicted values, colored by aridity\nggplot(best_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  scale_color_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Best Model Predictions vs. Observed Log Mean Flow\",\n       x = \"Observed logQmean\",\n       y = \"Predicted logQmean\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#Question 3: I would probably move forward with the neural-network model because it has the lowest RMSE and highest R-squared among the models indicating the best predicability performance.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(powerjoin)    # For merging data frames\nlibrary(tidymodels)\nlibrary(baguette)     # For neural network model\nlibrary(xgboost)      # For XGBoost\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(patchwork)\nsetwd(\"C:/Users/Cadre/git/Lab 6/Lab 6\")\n\n# Set seed for reproducibility\nset.seed(123)\n\n# --------------------------------------------------\n# 1. Download and Merge CAMELS Data\n# --------------------------------------------------\n# URLs for CAMELS data files\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"hydro\", \"soil\", \"topo\", \"vege\")\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files  &lt;- glue('data/camels_{types}.txt')\n\n# Download files if they don't exist\nwalk2(remote_files, local_files, ~{\n  if(!file.exists(.y)) download.file(.x, destfile = .y, quiet = TRUE)\n})\n\n# Read files into list of data frames\ncamels_list &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# Merge data frames on 'gauge_id'\ncamels &lt;- power_full_join(camels_list, by = 'gauge_id')\n\n# --------------------------------------------------\n# 2. Data Preparation & Splitting\n# --------------------------------------------------\n# Create log-transformed target variable (logQmean)\ncamels &lt;- camels %&gt;% mutate(logQmean = log(q_mean))\n\n# Split data into 75% training and 25% testing\ndata_split &lt;- initial_split(camels, prop = 0.75)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n# Create 10-fold cross-validation for training\ncv_folds &lt;- vfold_cv(train_data, v = 10)\n\n# --------------------------------------------------\n# 3. Define Recipe (Data Preprocessing)\n# --------------------------------------------------\n# Choose predictors that affect streamflow\nrecipe_model &lt;- recipe(logQmean ~ aridity + p_mean + elev_mean, data = train_data) %&gt;%\n  step_log(all_numeric_predictors()) %&gt;%    # Log-transform predictors\n  step_interact(terms = ~ aridity:p_mean) %&gt;%  # Add interaction between aridity and p_mean\n  step_normalize(all_numeric_predictors()) %&gt;% # Normalize predictors\n  step_naomit(all_predictors(), all_outcomes()) # Remove missing values\n\n# --------------------------------------------------\n# 4. Define Models\n# --------------------------------------------------\n# Random Forest Model\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# XGBoost Model\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = 6,\n  min_n = 10,\n  learn_rate = 0.01\n) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\n# Neural Network Model\nnn_model &lt;- bag_mlp(\n  hidden_units = 100,\n  epochs = 50  # Reduced epochs for faster processing\n) %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n# --------------------------------------------------\n# 5. Build Workflow Set\n# --------------------------------------------------\n# Combine the recipe and models into a workflow set\nwf_set &lt;- workflow_set(\n  preproc = list(recipe_model),\n  models  = list(\n    rf  = rf_model,\n    xgb = xgb_model,\n    nn  = nn_model\n  )\n)\n\n# --------------------------------------------------\n# 6. Fit Models via Resampling\n# --------------------------------------------------\n# Fit each model using 10-fold cross-validation\nwf_res &lt;- wf_set %&gt;% \n  workflow_map(\"fit_resamples\", \n               resamples = cv_folds, \n               seed = 123, \n               verbose = TRUE)\n\ni 1 of 3 resampling: recipe_rf\n\n\nâœ” 1 of 3 resampling: recipe_rf (2.7s)\n\n\ni 2 of 3 resampling: recipe_xgb\n\n\nâœ” 2 of 3 resampling: recipe_xgb (7.2s)\n\n\ni 3 of 3 resampling: recipe_nn\n\n\nâœ” 3 of 3 resampling: recipe_nn (41.2s)\n\n# --------------------------------------------------\n# 7. Evaluate and Compare Models\n# --------------------------------------------------\n# Collect performance metrics (e.g., RMSE, R-squared)\nmodel_metrics &lt;- wf_res %&gt;% collect_metrics()\nprint(model_metrics)\n\n# A tibble: 6 Ã— 9\n  wflow_id   .config        preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rmse    standard   0.442    10  0.0239\n2 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rsq     standard   0.866    10  0.0143\n3 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rmse    standard   0.465    10  0.0282\n4 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rsq     standard   0.852    10  0.0150\n5 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rmse    standard   0.431    10  0.0284\n6 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rsq     standard   0.874    10  0.0144\n\n# Rank models based on RMSE (lower is better)\nranked_results &lt;- rank_results(wf_res, rank_metric = \"rmse\", select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 6 Ã— 9\n  wflow_id   .config        .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn  Preprocessor1â€¦ rmse    0.431  0.0284    10 recipe       bag_â€¦     1\n2 recipe_nn  Preprocessor1â€¦ rsq     0.874  0.0144    10 recipe       bag_â€¦     1\n3 recipe_rf  Preprocessor1â€¦ rmse    0.442  0.0239    10 recipe       randâ€¦     2\n4 recipe_rf  Preprocessor1â€¦ rsq     0.866  0.0143    10 recipe       randâ€¦     2\n5 recipe_xgb Preprocessor1â€¦ rmse    0.465  0.0282    10 recipe       boosâ€¦     3\n6 recipe_xgb Preprocessor1â€¦ rsq     0.852  0.0150    10 recipe       boosâ€¦     3\n\n# --------------------------------------------------\n# 8. Evaluate Best Model on Test Data\n# --------------------------------------------------\n# Extract best model (based on previous ranking)\nbest_wf &lt;- wf_res %&gt;%\n  extract_workflow(\"recipe_xgb\") %&gt;%  # Use the best model id from ranked_results\n  fit(data = train_data)\n\n# Make predictions on the test data\nbest_preds &lt;- predict(best_wf, new_data = test_data) %&gt;% \n  bind_cols(test_data)\n\n# Evaluate performance on test data\ntest_metrics &lt;- metrics(best_preds, truth = logQmean, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.418\n2 rsq     standard       0.866\n3 mae     standard       0.248\n\n# Visualize observed vs. predicted values (colored by aridity)\nggplot(best_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  scale_color_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Test Data: Observed vs Predicted Log Mean Flow\",\n       x = \"Observed logQmean\",\n       y = \"Predicted logQmean\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#the model performs pretty good, closely matching observed values for wetter regions but it underpredicts streamflow in drier more arid basins, as seen by the scatter below the 1x1 line."
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "index",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.0.4     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.3.0 â”€â”€\nâœ” broom        1.0.7     âœ” rsample      1.2.1\nâœ” dials        1.4.0     âœ” tune         1.3.0\nâœ” infer        1.0.7     âœ” workflows    1.2.0\nâœ” modeldata    1.4.0     âœ” workflowsets 1.1.0\nâœ” parsnip      1.3.1     âœ” yardstick    1.3.2\nâœ” recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'infer' was built under R version 4.4.3\n\n\nWarning: package 'modeldata' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'workflowsets' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard() masks purrr::discard()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– recipes::fixed()  masks stringr::fixed()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– yardstick::spec() masks readr::spec()\nâœ– recipes::step()   masks stats::step()\n\nlibrary(tidypredict)\n\nWarning: package 'tidypredict' was built under R version 4.4.3\n\nlibrary(nnet)\n\nWarning: package 'nnet' was built under R version 4.4.3\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\n\n\nlibrary(tidyverse)\nlibrary(glue)\n\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'  # ðŸ”§ Define root BEFORE glue\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_clim.txt: cannot open destfile\n'data/camels_clim.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_geol.txt: cannot open destfile\n'data/camels_geol.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_soil.txt: cannot open destfile\n'data/camels_soil.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_topo.txt: cannot open destfile\n'data/camels_topo.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_vege.txt: cannot open destfile\n'data/camels_vege.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt: cannot open\ndestfile 'data/camels_hydro.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n#question 1: zero_q_freq represents the percentage of days with zero streamflow (0 mm/day) in a given period, providing insight into hydrological extremes and water behavior in the drainage area.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.4.3\n\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nlibrary(patchwork)\n\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Map for aridity\nmap_aridity &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) + \n  geom_point(aes(color = aridity)) + \n  scale_color_viridis_c() +\n  theme_minimal() + \n  labs(title = \"Aridity of Sites\", \n       color = \"Aridity\")\n\n# Map for p_mean (mean rainfall)\nmap_p_mean &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) + \n  geom_point(aes(color = p_mean)) + \n  scale_color_viridis_c() +\n  theme_minimal() + \n  labs(title = \"Mean Rainfall (p_mean) of Sites\", \n       color = \"Mean Rainfall\")\n\n# Combine maps using patchwork\nlibrary(patchwork)\ncombined_maps &lt;- map_aridity + map_p_mean + plot_layout(ncol = 1)\n\n# Display the combined maps\nprint(combined_maps)\n\n\n\n\n\n\n\n\n\n# Load libraries\nlibrary(tidymodels)\nlibrary(baguette)    # For neural network (bag_mlp)\nlibrary(xgboost)     # For XGBoost model\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\n# Set seed for reproducibility\nset.seed(123)\n\n# --- Data Preparation ---\n# Log-transform target variable 'q_mean' to create 'logQmean'\ncamels &lt;- camels %&gt;% mutate(logQmean = log(q_mean))\n\n# Split data: 80% for training, 20% for testing\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n# 5-fold cross-validation for faster processing\ncamels_cv &lt;- vfold_cv(camels_train, v = 5)\n\n# --- Preprocessing Recipe ---\n# Predict logQmean using aridity and p_mean\n# Log-transform predictors and add an interaction term\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%               # Log-transform predictors\n  step_interact(terms = ~ aridity:p_mean) %&gt;%   # Add interaction term\n  step_naomit(all_predictors(), all_outcomes()) # Remove rows with missing data\n\n# --- Model Specifications ---\n# 1. Linear Regression (baseline)\nlm_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# 2. Random Forest Model\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# 3. XGBoost Model\nxgb_model &lt;- boost_tree(\n  trees = 1000,   # Number of trees\n  tree_depth = 6, # Max depth of trees\n  min_n = 10,     # Min observations per node\n  learn_rate = 0.01 # Learning rate\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# 4. Neural Network (bag_mlp) Model\nnn_model &lt;- bag_mlp(\n  hidden_units = 100, # Number of hidden units\n  epochs = 50         # Reduced epochs for quicker testing\n) %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n# --- Create Workflow Set ---\n# Combine the recipe and all models into a workflow set\nwf_set &lt;- workflow_set(\n  preproc = list(rec),\n  models  = list(\n    lm  = lm_model,\n    rf  = rf_model,\n    xgb = xgb_model,\n    nn  = nn_model\n  )\n)\n\n# --- Fit Models with Resampling ---\n# Use 5-fold CV to fit each model in the workflow\nwf_res &lt;- wf_set %&gt;% \n  workflow_map(\"fit_resamples\", \n               resamples = camels_cv, \n               seed = 123, \n               verbose = TRUE)\n\ni 1 of 4 resampling: recipe_lm\n\n\nâœ” 1 of 4 resampling: recipe_lm (780ms)\n\n\ni 2 of 4 resampling: recipe_rf\n\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\nâœ” 2 of 4 resampling: recipe_rf (1.7s)\n\n\ni 3 of 4 resampling: recipe_xgb\n\n\nâœ” 3 of 4 resampling: recipe_xgb (4.5s)\n\n\ni 4 of 4 resampling: recipe_nn\n\n\nâœ” 4 of 4 resampling: recipe_nn (17.1s)\n\n# --- Evaluate Model Performance ---\n# Collect metrics (e.g., RMSE, R-squared, MAE)\nmodel_metrics &lt;- wf_res %&gt;% collect_metrics()\nprint(model_metrics)\n\n# A tibble: 8 Ã— 9\n  wflow_id   .config        preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_lm  Preprocessor1â€¦ recipe  lineâ€¦ rmse    standard   0.575     5  0.0220\n2 recipe_lm  Preprocessor1â€¦ recipe  lineâ€¦ rsq     standard   0.765     5  0.0216\n3 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rmse    standard   0.563     5  0.0241\n4 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rsq     standard   0.773     5  0.0278\n5 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rmse    standard   0.555     5  0.0197\n6 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rsq     standard   0.781     5  0.0217\n7 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rmse    standard   0.531     5  0.0289\n8 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rsq     standard   0.795     5  0.0309\n\n# Rank models based on RMSE (lower is better) or R-squared (higher is better)\nranked_results &lt;- rank_results(wf_res, rank_metric = \"rmse\", select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 8 Ã— 9\n  wflow_id   .config        .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn  Preprocessor1â€¦ rmse    0.531  0.0289     5 recipe       bag_â€¦     1\n2 recipe_nn  Preprocessor1â€¦ rsq     0.795  0.0309     5 recipe       bag_â€¦     1\n3 recipe_xgb Preprocessor1â€¦ rmse    0.555  0.0197     5 recipe       boosâ€¦     2\n4 recipe_xgb Preprocessor1â€¦ rsq     0.781  0.0217     5 recipe       boosâ€¦     2\n5 recipe_rf  Preprocessor1â€¦ rmse    0.563  0.0241     5 recipe       randâ€¦     3\n6 recipe_rf  Preprocessor1â€¦ rsq     0.773  0.0278     5 recipe       randâ€¦     3\n7 recipe_lm  Preprocessor1â€¦ rmse    0.575  0.0220     5 recipe       lineâ€¦     4\n8 recipe_lm  Preprocessor1â€¦ rsq     0.765  0.0216     5 recipe       lineâ€¦     4\n\n# --- Extract the Best Model ---\n# Assume XGBoost is the best based on RMSE (adjust if necessary)\nbest_wf &lt;- wf_res %&gt;%\n  extract_workflow(\"recipe_xgb\") %&gt;%  # Replace with the best model id if different\n  fit(data = camels_train)\n\n# --- Make Predictions on Test Data ---\nbest_preds &lt;- predict(best_wf, new_data = camels_test) %&gt;% \n  bind_cols(camels_test)\n\n# Evaluate the best model's performance on the test data\ntest_metrics &lt;- metrics(best_preds, truth = logQmean, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.652\n2 rsq     standard       0.694\n3 mae     standard       0.389\n\n# --- Visualize: Observed vs. Predicted ---\n# Plot observed vs predicted values, colored by aridity\nggplot(best_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  scale_color_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Best Model Predictions vs. Observed Log Mean Flow\",\n       x = \"Observed logQmean\",\n       y = \"Predicted logQmean\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#Question 3: I would probably move forward with the neural-network model because it has the lowest RMSE and highest R-squared among the models indicating the best predicability performance.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(powerjoin)    # For merging data frames\nlibrary(tidymodels)\nlibrary(baguette)     # For neural network model\nlibrary(xgboost)      # For XGBoost\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(patchwork)\nsetwd(\"C:/Users/Cadre/git/Lab 6/Lab 6\")\n\n# Set seed for reproducibility\nset.seed(123)\n\n# --------------------------------------------------\n# 1. Download and Merge CAMELS Data\n# --------------------------------------------------\n# URLs for CAMELS data files\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"hydro\", \"soil\", \"topo\", \"vege\")\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files  &lt;- glue('data/camels_{types}.txt')\n\n# Download files if they don't exist\nwalk2(remote_files, local_files, ~{\n  if(!file.exists(.y)) download.file(.x, destfile = .y, quiet = TRUE)\n})\n\n# Read files into list of data frames\ncamels_list &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# Merge data frames on 'gauge_id'\ncamels &lt;- power_full_join(camels_list, by = 'gauge_id')\n\n# --------------------------------------------------\n# 2. Data Preparation & Splitting\n# --------------------------------------------------\n# Create log-transformed target variable (logQmean)\ncamels &lt;- camels %&gt;% mutate(logQmean = log(q_mean))\n\n# Split data into 75% training and 25% testing\ndata_split &lt;- initial_split(camels, prop = 0.75)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n# Create 10-fold cross-validation for training\ncv_folds &lt;- vfold_cv(train_data, v = 10)\n\n# --------------------------------------------------\n# 3. Define Recipe (Data Preprocessing)\n# --------------------------------------------------\n# Choose predictors that affect streamflow\nrecipe_model &lt;- recipe(logQmean ~ aridity + p_mean + elev_mean, data = train_data) %&gt;%\n  step_log(all_numeric_predictors()) %&gt;%    # Log-transform predictors\n  step_interact(terms = ~ aridity:p_mean) %&gt;%  # Add interaction between aridity and p_mean\n  step_normalize(all_numeric_predictors()) %&gt;% # Normalize predictors\n  step_naomit(all_predictors(), all_outcomes()) # Remove missing values\n\n# --------------------------------------------------\n# 4. Define Models\n# --------------------------------------------------\n# Random Forest Model\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# XGBoost Model\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = 6,\n  min_n = 10,\n  learn_rate = 0.01\n) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\n# Neural Network Model\nnn_model &lt;- bag_mlp(\n  hidden_units = 100,\n  epochs = 50  # Reduced epochs for faster processing\n) %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n# --------------------------------------------------\n# 5. Build Workflow Set\n# --------------------------------------------------\n# Combine the recipe and models into a workflow set\nwf_set &lt;- workflow_set(\n  preproc = list(recipe_model),\n  models  = list(\n    rf  = rf_model,\n    xgb = xgb_model,\n    nn  = nn_model\n  )\n)\n\n# --------------------------------------------------\n# 6. Fit Models via Resampling\n# --------------------------------------------------\n# Fit each model using 10-fold cross-validation\nwf_res &lt;- wf_set %&gt;% \n  workflow_map(\"fit_resamples\", \n               resamples = cv_folds, \n               seed = 123, \n               verbose = TRUE)\n\ni 1 of 3 resampling: recipe_rf\n\n\nâœ” 1 of 3 resampling: recipe_rf (2.7s)\n\n\ni 2 of 3 resampling: recipe_xgb\n\n\nâœ” 2 of 3 resampling: recipe_xgb (7.6s)\n\n\ni 3 of 3 resampling: recipe_nn\n\n\nâœ” 3 of 3 resampling: recipe_nn (43.4s)\n\n# --------------------------------------------------\n# 7. Evaluate and Compare Models\n# --------------------------------------------------\n# Collect performance metrics (e.g., RMSE, R-squared)\nmodel_metrics &lt;- wf_res %&gt;% collect_metrics()\nprint(model_metrics)\n\n# A tibble: 6 Ã— 9\n  wflow_id   .config        preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rmse    standard   0.442    10  0.0239\n2 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rsq     standard   0.866    10  0.0143\n3 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rmse    standard   0.465    10  0.0282\n4 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rsq     standard   0.852    10  0.0150\n5 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rmse    standard   0.431    10  0.0284\n6 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rsq     standard   0.874    10  0.0144\n\n# Rank models based on RMSE (lower is better)\nranked_results &lt;- rank_results(wf_res, rank_metric = \"rmse\", select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 6 Ã— 9\n  wflow_id   .config        .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn  Preprocessor1â€¦ rmse    0.431  0.0284    10 recipe       bag_â€¦     1\n2 recipe_nn  Preprocessor1â€¦ rsq     0.874  0.0144    10 recipe       bag_â€¦     1\n3 recipe_rf  Preprocessor1â€¦ rmse    0.442  0.0239    10 recipe       randâ€¦     2\n4 recipe_rf  Preprocessor1â€¦ rsq     0.866  0.0143    10 recipe       randâ€¦     2\n5 recipe_xgb Preprocessor1â€¦ rmse    0.465  0.0282    10 recipe       boosâ€¦     3\n6 recipe_xgb Preprocessor1â€¦ rsq     0.852  0.0150    10 recipe       boosâ€¦     3\n\n# --------------------------------------------------\n# 8. Evaluate Best Model on Test Data\n# --------------------------------------------------\n# Extract best model (based on previous ranking)\nbest_wf &lt;- wf_res %&gt;%\n  extract_workflow(\"recipe_xgb\") %&gt;%  # Use the best model id from ranked_results\n  fit(data = train_data)\n\n# Make predictions on the test data\nbest_preds &lt;- predict(best_wf, new_data = test_data) %&gt;% \n  bind_cols(test_data)\n\n# Evaluate performance on test data\ntest_metrics &lt;- metrics(best_preds, truth = logQmean, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.418\n2 rsq     standard       0.866\n3 mae     standard       0.248\n\n# Visualize observed vs. predicted values (colored by aridity)\nggplot(best_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  scale_color_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Test Data: Observed vs Predicted Log Mean Flow\",\n       x = \"Observed logQmean\",\n       y = \"Predicted logQmean\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#the model performs pretty good, closely matching observed values for wetter regions but it underpredicts streamflow in drier more arid basins, as seen by the scatter below the 1x1 line."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "index",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.0.4     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.3.0 â”€â”€\nâœ” broom        1.0.7     âœ” rsample      1.2.1\nâœ” dials        1.4.0     âœ” tune         1.3.0\nâœ” infer        1.0.7     âœ” workflows    1.2.0\nâœ” modeldata    1.4.0     âœ” workflowsets 1.1.0\nâœ” parsnip      1.3.1     âœ” yardstick    1.3.2\nâœ” recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'infer' was built under R version 4.4.3\n\n\nWarning: package 'modeldata' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'workflowsets' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard() masks purrr::discard()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– recipes::fixed()  masks stringr::fixed()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– yardstick::spec() masks readr::spec()\nâœ– recipes::step()   masks stats::step()\n\nlibrary(tidypredict)\n\nWarning: package 'tidypredict' was built under R version 4.4.3\n\nlibrary(nnet)\n\nWarning: package 'nnet' was built under R version 4.4.3\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\n\n\nlibrary(tidyverse)\nlibrary(glue)\n\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'  # ðŸ”§ Define root BEFORE glue\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_clim.txt: cannot open destfile\n'data/camels_clim.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_geol.txt: cannot open destfile\n'data/camels_geol.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_soil.txt: cannot open destfile\n'data/camels_soil.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_topo.txt: cannot open destfile\n'data/camels_topo.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_vege.txt: cannot open destfile\n'data/camels_vege.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt: cannot open\ndestfile 'data/camels_hydro.txt', reason 'No such file or directory'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n#question 1: zero_q_freq represents the percentage of days with zero streamflow (0 mm/day) in a given period, providing insight into hydrological extremes and water behavior in the drainage area.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.4.3\n\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nlibrary(patchwork)\n\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Map for aridity\nmap_aridity &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) + \n  geom_point(aes(color = aridity)) + \n  scale_color_viridis_c() +\n  theme_minimal() + \n  labs(title = \"Aridity of Sites\", \n       color = \"Aridity\")\n\n# Map for p_mean (mean rainfall)\nmap_p_mean &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) + \n  geom_point(aes(color = p_mean)) + \n  scale_color_viridis_c() +\n  theme_minimal() + \n  labs(title = \"Mean Rainfall (p_mean) of Sites\", \n       color = \"Mean Rainfall\")\n\n# Combine maps using patchwork\nlibrary(patchwork)\ncombined_maps &lt;- map_aridity + map_p_mean + plot_layout(ncol = 1)\n\n# Display the combined maps\nprint(combined_maps)\n\n\n\n\n\n\n\n\n\n# Load libraries\nlibrary(tidymodels)\nlibrary(baguette)    # For neural network (bag_mlp)\nlibrary(xgboost)     # For XGBoost model\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\n# Set seed for reproducibility\nset.seed(123)\n\n# --- Data Preparation ---\n# Log-transform target variable 'q_mean' to create 'logQmean'\ncamels &lt;- camels %&gt;% mutate(logQmean = log(q_mean))\n\n# Split data: 80% for training, 20% for testing\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n# 5-fold cross-validation for faster processing\ncamels_cv &lt;- vfold_cv(camels_train, v = 5)\n\n# --- Preprocessing Recipe ---\n# Predict logQmean using aridity and p_mean\n# Log-transform predictors and add an interaction term\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%               # Log-transform predictors\n  step_interact(terms = ~ aridity:p_mean) %&gt;%   # Add interaction term\n  step_naomit(all_predictors(), all_outcomes()) # Remove rows with missing data\n\n# --- Model Specifications ---\n# 1. Linear Regression (baseline)\nlm_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# 2. Random Forest Model\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# 3. XGBoost Model\nxgb_model &lt;- boost_tree(\n  trees = 1000,   # Number of trees\n  tree_depth = 6, # Max depth of trees\n  min_n = 10,     # Min observations per node\n  learn_rate = 0.01 # Learning rate\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# 4. Neural Network (bag_mlp) Model\nnn_model &lt;- bag_mlp(\n  hidden_units = 100, # Number of hidden units\n  epochs = 50         # Reduced epochs for quicker testing\n) %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n# --- Create Workflow Set ---\n# Combine the recipe and all models into a workflow set\nwf_set &lt;- workflow_set(\n  preproc = list(rec),\n  models  = list(\n    lm  = lm_model,\n    rf  = rf_model,\n    xgb = xgb_model,\n    nn  = nn_model\n  )\n)\n\n# --- Fit Models with Resampling ---\n# Use 5-fold CV to fit each model in the workflow\nwf_res &lt;- wf_set %&gt;% \n  workflow_map(\"fit_resamples\", \n               resamples = camels_cv, \n               seed = 123, \n               verbose = TRUE)\n\ni 1 of 4 resampling: recipe_lm\n\n\nâœ” 1 of 4 resampling: recipe_lm (500ms)\n\n\ni 2 of 4 resampling: recipe_rf\n\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\nâœ” 2 of 4 resampling: recipe_rf (1.2s)\n\n\ni 3 of 4 resampling: recipe_xgb\n\n\nâœ” 3 of 4 resampling: recipe_xgb (3.8s)\n\n\ni 4 of 4 resampling: recipe_nn\n\n\nâœ” 4 of 4 resampling: recipe_nn (17.3s)\n\n# --- Evaluate Model Performance ---\n# Collect metrics (e.g., RMSE, R-squared, MAE)\nmodel_metrics &lt;- wf_res %&gt;% collect_metrics()\nprint(model_metrics)\n\n# A tibble: 8 Ã— 9\n  wflow_id   .config        preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_lm  Preprocessor1â€¦ recipe  lineâ€¦ rmse    standard   0.575     5  0.0220\n2 recipe_lm  Preprocessor1â€¦ recipe  lineâ€¦ rsq     standard   0.765     5  0.0216\n3 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rmse    standard   0.563     5  0.0241\n4 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rsq     standard   0.773     5  0.0278\n5 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rmse    standard   0.555     5  0.0197\n6 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rsq     standard   0.781     5  0.0217\n7 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rmse    standard   0.531     5  0.0289\n8 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rsq     standard   0.795     5  0.0309\n\n# Rank models based on RMSE (lower is better) or R-squared (higher is better)\nranked_results &lt;- rank_results(wf_res, rank_metric = \"rmse\", select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 8 Ã— 9\n  wflow_id   .config        .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn  Preprocessor1â€¦ rmse    0.531  0.0289     5 recipe       bag_â€¦     1\n2 recipe_nn  Preprocessor1â€¦ rsq     0.795  0.0309     5 recipe       bag_â€¦     1\n3 recipe_xgb Preprocessor1â€¦ rmse    0.555  0.0197     5 recipe       boosâ€¦     2\n4 recipe_xgb Preprocessor1â€¦ rsq     0.781  0.0217     5 recipe       boosâ€¦     2\n5 recipe_rf  Preprocessor1â€¦ rmse    0.563  0.0241     5 recipe       randâ€¦     3\n6 recipe_rf  Preprocessor1â€¦ rsq     0.773  0.0278     5 recipe       randâ€¦     3\n7 recipe_lm  Preprocessor1â€¦ rmse    0.575  0.0220     5 recipe       lineâ€¦     4\n8 recipe_lm  Preprocessor1â€¦ rsq     0.765  0.0216     5 recipe       lineâ€¦     4\n\n# --- Extract the Best Model ---\n# Assume XGBoost is the best based on RMSE (adjust if necessary)\nbest_wf &lt;- wf_res %&gt;%\n  extract_workflow(\"recipe_xgb\") %&gt;%  # Replace with the best model id if different\n  fit(data = camels_train)\n\n# --- Make Predictions on Test Data ---\nbest_preds &lt;- predict(best_wf, new_data = camels_test) %&gt;% \n  bind_cols(camels_test)\n\n# Evaluate the best model's performance on the test data\ntest_metrics &lt;- metrics(best_preds, truth = logQmean, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.652\n2 rsq     standard       0.694\n3 mae     standard       0.389\n\n# --- Visualize: Observed vs. Predicted ---\n# Plot observed vs predicted values, colored by aridity\nggplot(best_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  scale_color_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Best Model Predictions vs. Observed Log Mean Flow\",\n       x = \"Observed logQmean\",\n       y = \"Predicted logQmean\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#Question 3: I would probably move forward with the neural-network model because it has the lowest RMSE and highest R-squared among the models indicating the best predicability performance.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(powerjoin)    # For merging data frames\nlibrary(tidymodels)\nlibrary(baguette)     # For neural network model\nlibrary(xgboost)      # For XGBoost\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(patchwork)\nsetwd(\"C:/Users/Cadre/git/Lab 6/Lab 6\")\n\n# Set seed for reproducibility\nset.seed(123)\n\n# --------------------------------------------------\n# 1. Download and Merge CAMELS Data\n# --------------------------------------------------\n# URLs for CAMELS data files\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"hydro\", \"soil\", \"topo\", \"vege\")\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files  &lt;- glue('data/camels_{types}.txt')\n\n# Download files if they don't exist\nwalk2(remote_files, local_files, ~{\n  if(!file.exists(.y)) download.file(.x, destfile = .y, quiet = TRUE)\n})\n\n# Read files into list of data frames\ncamels_list &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# Merge data frames on 'gauge_id'\ncamels &lt;- power_full_join(camels_list, by = 'gauge_id')\n\n# --------------------------------------------------\n# 2. Data Preparation & Splitting\n# --------------------------------------------------\n# Create log-transformed target variable (logQmean)\ncamels &lt;- camels %&gt;% mutate(logQmean = log(q_mean))\n\n# Split data into 75% training and 25% testing\ndata_split &lt;- initial_split(camels, prop = 0.75)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n# Create 10-fold cross-validation for training\ncv_folds &lt;- vfold_cv(train_data, v = 10)\n\n# --------------------------------------------------\n# 3. Define Recipe (Data Preprocessing)\n# --------------------------------------------------\n# Choose predictors that affect streamflow\nrecipe_model &lt;- recipe(logQmean ~ aridity + p_mean + elev_mean, data = train_data) %&gt;%\n  step_log(all_numeric_predictors()) %&gt;%    # Log-transform predictors\n  step_interact(terms = ~ aridity:p_mean) %&gt;%  # Add interaction between aridity and p_mean\n  step_normalize(all_numeric_predictors()) %&gt;% # Normalize predictors\n  step_naomit(all_predictors(), all_outcomes()) # Remove missing values\n\n# --------------------------------------------------\n# 4. Define Models\n# --------------------------------------------------\n# Random Forest Model\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# XGBoost Model\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = 6,\n  min_n = 10,\n  learn_rate = 0.01\n) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\n# Neural Network Model\nnn_model &lt;- bag_mlp(\n  hidden_units = 100,\n  epochs = 50  # Reduced epochs for faster processing\n) %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n# --------------------------------------------------\n# 5. Build Workflow Set\n# --------------------------------------------------\n# Combine the recipe and models into a workflow set\nwf_set &lt;- workflow_set(\n  preproc = list(recipe_model),\n  models  = list(\n    rf  = rf_model,\n    xgb = xgb_model,\n    nn  = nn_model\n  )\n)\n\n# --------------------------------------------------\n# 6. Fit Models via Resampling\n# --------------------------------------------------\n# Fit each model using 10-fold cross-validation\nwf_res &lt;- wf_set %&gt;% \n  workflow_map(\"fit_resamples\", \n               resamples = cv_folds, \n               seed = 123, \n               verbose = TRUE)\n\ni 1 of 3 resampling: recipe_rf\n\n\nâœ” 1 of 3 resampling: recipe_rf (2.8s)\n\n\ni 2 of 3 resampling: recipe_xgb\n\n\nâœ” 2 of 3 resampling: recipe_xgb (7.7s)\n\n\ni 3 of 3 resampling: recipe_nn\n\n\nâœ” 3 of 3 resampling: recipe_nn (44.6s)\n\n# --------------------------------------------------\n# 7. Evaluate and Compare Models\n# --------------------------------------------------\n# Collect performance metrics (e.g., RMSE, R-squared)\nmodel_metrics &lt;- wf_res %&gt;% collect_metrics()\nprint(model_metrics)\n\n# A tibble: 6 Ã— 9\n  wflow_id   .config        preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rmse    standard   0.442    10  0.0239\n2 recipe_rf  Preprocessor1â€¦ recipe  randâ€¦ rsq     standard   0.866    10  0.0143\n3 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rmse    standard   0.465    10  0.0282\n4 recipe_xgb Preprocessor1â€¦ recipe  boosâ€¦ rsq     standard   0.852    10  0.0150\n5 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rmse    standard   0.431    10  0.0284\n6 recipe_nn  Preprocessor1â€¦ recipe  bag_â€¦ rsq     standard   0.874    10  0.0144\n\n# Rank models based on RMSE (lower is better)\nranked_results &lt;- rank_results(wf_res, rank_metric = \"rmse\", select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 6 Ã— 9\n  wflow_id   .config        .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn  Preprocessor1â€¦ rmse    0.431  0.0284    10 recipe       bag_â€¦     1\n2 recipe_nn  Preprocessor1â€¦ rsq     0.874  0.0144    10 recipe       bag_â€¦     1\n3 recipe_rf  Preprocessor1â€¦ rmse    0.442  0.0239    10 recipe       randâ€¦     2\n4 recipe_rf  Preprocessor1â€¦ rsq     0.866  0.0143    10 recipe       randâ€¦     2\n5 recipe_xgb Preprocessor1â€¦ rmse    0.465  0.0282    10 recipe       boosâ€¦     3\n6 recipe_xgb Preprocessor1â€¦ rsq     0.852  0.0150    10 recipe       boosâ€¦     3\n\n# --------------------------------------------------\n# 8. Evaluate Best Model on Test Data\n# --------------------------------------------------\n# Extract best model (based on previous ranking)\nbest_wf &lt;- wf_res %&gt;%\n  extract_workflow(\"recipe_xgb\") %&gt;%  # Use the best model id from ranked_results\n  fit(data = train_data)\n\n# Make predictions on the test data\nbest_preds &lt;- predict(best_wf, new_data = test_data) %&gt;% \n  bind_cols(test_data)\n\n# Evaluate performance on test data\ntest_metrics &lt;- metrics(best_preds, truth = logQmean, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.418\n2 rsq     standard       0.866\n3 mae     standard       0.248\n\n# Visualize observed vs. predicted values (colored by aridity)\nggplot(best_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  scale_color_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Test Data: Observed vs Predicted Log Mean Flow\",\n       x = \"Observed logQmean\",\n       y = \"Predicted logQmean\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#the model performs pretty good, closely matching observed values for wetter regions but it underpredicts streamflow in drier more arid basins, as seen by the scatter below the 1x1 line."
  }
]